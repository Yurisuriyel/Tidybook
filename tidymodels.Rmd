---
title: "tidymodels"
author: "Linze Yu"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    self_contained: TRUE
    default_style: "light"
    downcute_theme: "default"
    fig_width: 7
    fig_height: 5
    use_bookdown: TRUE
    gallery: TRUE
    lightbox: TRUE
    highlight: tango
editor_options: 
  markdown: 
    wrap: 72
---

```{r, 代码全局设置, echo = F}
knitr::opts_chunk$set(warning = F, error = F, message = F, prompt = F, comment = "", echo = T, dpi = 200, collapse = F, fig.align = "left", dev = "png", cache = T)
# , eval = F代码显示不运行
# , include = F代码运行, 不显示代码和结果
# , echo = T 显示代码块
# , prompt = T使用>开始代码
# , comment = ""结果不使用##
# , collapse = T#代码合并结果
# , out.width/out.height = 0.8缩放
# , fig.align = "left","center","right"对齐方式
# , dev = "pdf","png","svg","jpeg"记录设备
# , cache = T
```

## 加载包
```{r 加载包}
library("YuriR") #
library("tidymodels") #
library("keras")
# library("mlr3verse") #


library("broom.mixed") # 用于将贝叶斯模型转换为整洁的tibbles
library("dotwhisker") # 用于可视化回归结果
# remotes::install_github("fsolt/dotwhisker")

library("nycflights13")
library("skimr")

library("ranger")
library("modeldata")

library("randomForest") # 随机森林
# library("glmnet") #
library("ranger") #

library("rstanarm") # 用于贝叶斯应用回归建模
library("MASS") #
library("nnet") #
# library("survival")#
library("AER") #
# library("glmnet") #

library("caret")
library("pROC")
```

# tidymodels
rsample:数据划分和重抽样
recipes:用于特征工程的数据预处理
parsnip:对一系列模型进行建模的统一接口
broom:将得到的模型信息转换成整洁的格式
tune:调整超参数，对预处理过程进行优化
dials:创建,管理超参数和参数格点
yardstick:使用性能指标度量模型的有效性
workflows:将模型的预处理,建模和后处理整合在一起

infer:统计推断
corrr:整洁的相关矩阵
discrim:判别分析
bayesmodels:贝叶斯模型的Tidymodels扩展
rstanarm:R包用于贝叶斯应用回归建模

|parsnip函数|模型/算法|引擎|
|:-:|:-:|:-:|
|`boost_tree()`|boost树|`xgboost`(默认),`C5.0`,`spark`|
|`decision_tree()`|决策树|`rpart`(默认),`C5.0`(只用于聚类),`spark`|
|`linear_reg()`|线性回归|`lm`(默认),`glmnet`,`stan`,`spark`,`keras`|
|`logistic_reg()`|逻辑回归|`glm`(默认),`glmnet`,`stan`,`spark`,`keras`|
|`mars()`|多元适应性回归样条|`earth`|
|`mlp()`|单层神经网络|`nnet`(默认),`keras`|
|`multinom_reg()`|多项式回归|`glmnet`(默认),`nnet`,`stan`,`keras`|
|`nearest_neighbor()`|K近邻聚类|`kknn`|
|`rand_forest()`|随机森林|`ranger`(默认),`randomForest`,`spark`|
|`surv_reg()`|生存分析|`survival`(默认),`flexsurv`|
|`svm_poly()`|多项式支持向量机|`kernlab`|
|`svm_rbf()`|径向基函数支持向量机|`kernlab`(默认),`liquidSVM`|

# learn
## 建立模型1---海胆
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/urchins.xlsx") %>%
  setNames(c("food_regime", "initial_volume", "width")) %>%
  mutate(food_regime = factor(food_regime, levels = c("Initial", "Low", "High"))) -> urchins
```

```{r}
urchins %>%
  ggplot(aes(x = initial_volume, y = width, col = food_regime, fill = food_regime)) +
  geom_point(shape = 21, size = 2, stroke = 1) +
  geom_smooth(method = lm, se = F) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme +
  theme(legend.position = (c(0.2, 0.95)))
```

### 建立并拟合一个模型
(parsnip)  
一个标准的双因素方差分析(ANOVA)模型对这个数据集是有意义的,因为我们有一个连续的预测因子和一个分类预测因子。  
```{r}
lm_mod <- linear_reg() %>%
  set_engine("lm") # 模型引擎

lm_fit <- lm_mod %>%
  fit(width ~ initial_volume * food_regime, data = urchins)
lm_fit

tidy(lm_fit)
```

点须图  
(dotwhisker)
```{r}
tidy(lm_fit) %>%
  dwplot(
    dot_args = list(size = 2, color = "#FC766A", shape = 21, stroke = 1),
    whisker_args = list(size = 1, color = "#696AAD"),
    vline = geom_vline(size = 1, xintercept = 0, colour = "#96999C", linetype = 2)
  ) +
  Yuri_theme
```

预测-预测值的类型是标准化的
```{r}
new_points <- expand.grid(
  initial_volume = 20,
  food_regime = c("Initial", "Low", "High")
)
new_points
```

```{r}
mean_pred <- predict(lm_fit, new_data = new_points)
mean_pred
```

```{r}
conf_int_pred <- predict(lm_fit,
  new_data = new_points,
  type = "conf_int"
)
conf_int_pred
```

```{r}
plot_data <-
  new_points %>%
  bind_cols(mean_pred) %>%
  bind_cols(conf_int_pred)
```

```{r}
ggplot(plot_data, aes(x = food_regime)) +
  geom_point(aes(y = .pred), size = 3, shape = 21, stroke = 1, color = "#FC766A") +
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), size = 1, width = .2, color = "#696AAD") +
  labs(y = "urchin size") +
  Yuri_theme
```

使用不同的引擎建模
(rstanarm)
```{r}
prior_dist <- rstanarm::student_t(df = 1)

set.seed(2022)
# 制作模型
bayes_mod <- linear_reg() %>%
  set_engine("stan",
    prior_intercept = prior_dist,
    prior = prior_dist
  )
```

```{r}
# 训练模型
bayes_fit <-
  bayes_mod %>%
  fit(width ~ initial_volume * food_regime, data = urchins)

print(bayes_fit, digits = 5)
tidy(bayes_fit, conf.int = TRUE)
```

```{r}
bayes_plot_data <-
  new_points %>%
  bind_cols(predict(bayes_fit, new_data = new_points)) %>%
  bind_cols(predict(bayes_fit, new_data = new_points, type = "conf_int"))

ggplot(bayes_plot_data, aes(x = food_regime)) +
  geom_point(aes(y = .pred), size = 3, shape = 21, stroke = 1, color = "#FC766A") +
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), size = 1, width = .2, color = "#696AAD") +
  labs(y = "urchin size", title = "Bayesian model with t(1) prior distribution") +
  Yuri_theme
```

## 建立模型2---纽约市的航班数据
(parsnip)  
预测飞机是否晚到30分钟以上,这个数据集包含2013年在纽约市附近出发的325,819个航班的信息(csv)

```{r}
import("D:/desktop/book/tidybook/data/tidymodels/flight_data.csv") -> flight_data

flight_data %>%
  count(arr_delay) %>%
  mutate(prop = n / sum(n))
```

```{r}
glimpse(flight_data)
flight_data %>%
  skimr::skim(dest, carrier)
```

### 数据拆分
```{r}
set.seed(2022)
data_split <- initial_split(flight_data, prop = 3 / 4)
train_data <- training(data_split)
test_data <- testing(data_split)
```

### 预处理
```{r}
flights_rec <-
  recipe(arr_delay ~ ., # .使用全部变量
    data = train_data
  ) %>%
  update_role(flight, time_hour, new_role = "ID")
summary(flights_rec)
```

### 特征工程
一周中的一天。
月份，以及
该日期是否对应于一个假期。

```{r}
flights_rec <-
  recipe(arr_delay ~ ., data = train_data) %>%
  update_role(flight, time_hour, new_role = "ID") %>%
  step_date(date, # 相应的星期和月份的日期
    features = c("dow", "month")
  ) %>%
  step_holiday(date, # 表明当前日期是否是一个假期
    holidays = timeDate::listHolidays("US"),
    keep_original_cols = F
  ) %>%
  step_dummy(all_nominal_predictors()) # 创建虚拟变量
```

```{r}
test_data %>%
  distinct(dest) %>%
  anti_join(train_data)
```

```{r}
flights_rec <-
  recipe(arr_delay ~ ., data = train_data) %>%
  update_role(flight, time_hour, new_role = "ID") %>%
  step_date(date, features = c("dow", "month")) %>%
  step_holiday(date,
    holidays = timeDate::listHolidays("US"),
    keep_original_cols = FALSE
  ) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())
```

### 拟合一个模型
```{r}
lr_mod <-
  logistic_reg() %>%
  set_engine("glm")
```

```{r}
flights_wflow <-
  workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(flights_rec)

flights_wflow
```

```{r}
flights_fit <-
  flights_wflow %>%
  fit(data = train_data)
```

```{r}
flights_fit %>%
  extract_fit_parsnip() %>%
  tidy()
```

```{r}
predict(flights_fit, test_data)
```

```{r}
augment(flights_fit, test_data) -> flights_aug
```

## 建立模型3---cells
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/cells.csv") -> cells
```

```{r}
cells %>%
  count(class) %>%
  mutate(prop = n / sum(n))
```

```{r}
set.seed(2022)
cell_split <- initial_split(cells %>% select_mix(-c("case")), strata = class)
```

```{r}
cell_train <- training(cell_split)
cell_test <- testing(cell_split)

nrow(cell_train)
nrow(cell_train) / nrow(cells)
cell_train %>%
  count(class) %>%
  mutate(prop = n / sum(n))
cell_test %>%
  count(class) %>%
  mutate(prop = n / sum(n))
```

### 建模
```{r}
rf_mod <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")
```






## 具有整洁数据原则的相关性和回归基础知识
(tidymodels)

## 具有整洁数据原理的 K 均值聚类(一种无监督学习)
(tidymodels)  
鸢尾花  
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/iris.xlsx") %>%
  select_mix(Sepal.Length, Sepal.Width, Species) %>%
  data.frame() -> data
```

```{r}   
data %>%
  ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) +
  geom_point(alpha = 1, size = 3, shape = 21, stroke = 1) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
```

```{r}
data %>%
  select_mix(-c("Species")) -> points
kmeans(points, centers = 3) -> kclust # 在一个矩阵上执行K均值聚类
kclust
summary(kclust) # 查看信息
kclust$cluster # 查看聚类的结果,也就是每个点聚到了那个类
kclust$centers # 查看中心点(聚类点)
augment(kclust, points) # 将点分类添加到原始数据集
tidy(kclust) %>% # 每个集群级别进行汇总
  data.table()
glance(kclust) # 提取单行摘要
```

### 探索性聚类
探索从1到24的不同选择对此聚类的影响
(furrr)
```{r}
tibble(k = 1:24) %>%
  mutate(
    kclust = future_map(k, ~ kmeans(points, .x)),
    tidied = future_map(kclust, tidy),
    glanced = future_map(kclust, glance),
    augmented = future_map(kclust, augment, points)
  ) -> kclusts
kclusts
```

```{r}
kclusts %>%
  unnest(cols = c(tidied)) -> clusters
clusters
kclusts %>%
  unnest(cols = c(augmented)) -> assignments
assignments
kclusts %>%
  unnest(cols = c(glanced)) -> clusterings
clusterings
```

```{r}
assignments %>%
  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point(aes(color = .cluster), shape = 21, fill = NA, stroke = 0.5) +
  geom_point(data = clusters, size = 10, shape = "*", color = 1) +
  facet_wrap(~k, ncol = 4) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
```

```{r}
clusterings %>%
  ggplot(aes(k, tot.withinss)) +
  geom_line(size = 2, linetype = 3, color = "#FC766A") +
  geom_point(size = 5, shape = 21, fill = NA, stroke = 2, color = "#34558B") +
  scale_y_continuous(breaks = seq(0, 200, by = 25), minor_breaks = seq(0, 200, by = 5)) +
  scale_x_continuous(breaks = seq(0, 13, by = 1), minor_breaks = seq(0, 0, by = 0)) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
```

## 引导重采样和整洁的回归模型
应用自举重采样来估计模型参数中的不确定性。
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/mtcars.xlsx") -> data
data %>%
  ggplot(aes(x = mpg, y = wt)) +
  geom_point(size = 2, shape = 21, stroke = 1, color = "#34558B") +
  Yuri_theme
```

非线性最小二乘法来拟合模型
```{r}
nlsfit <- nls(mpg ~ k / wt + b, data = data, start = list(k = 1, b = 0))
summary(nlsfit)
tidy(nlsfit)
data %>%
  ggplot(aes(x = wt, y = mpg)) +
  geom_point(size = 2, shape = 21, stroke = 1, color = "#34558B") +
  geom_line(aes(y = predict(nlsfit)), color = "#FC766A", size = 1, linetype = 3) +
  Yuri_theme
```
自举模型
```{r}
set.seed(2022)
boots <- bootstraps(data, times = 2000, apparent = T)
boots
```

```{r}
fit_nls_on_bootstrap <- function(split) {
  nls(mpg ~ k / wt + b, analysis(split), start = list(k = 1, b = 0))
}

boot_models <-
  boots %>%
  mutate(
    model = future_map(splits, fit_nls_on_bootstrap),
    coef_info = future_map(model, tidy)
  )

boot_coefs <-
  boot_models %>%
  unnest(coef_info)
boot_coefs
```
置信区间
```{r}
percentile_intervals <- int_pctl(boot_models, coef_info) # 百分位方法
percentile_intervals
```

```{r}
boot_coefs %>%
  ggplot(aes(estimate)) +
  geom_histogram(bins = 30, color = NA, alpha = 0.3, fill = "#00997B") +
  facet_wrap(~term, scales = "free") +
  geom_vline(aes(xintercept = .lower), data = percentile_intervals, color = "#34558B") +
  geom_vline(aes(xintercept = .upper), data = percentile_intervals, color = "#34558B") +
  Yuri_theme
```

可能的模型拟合
```{r}
boot_aug <-
  boot_models %>%
  sample_n(200) %>%
  mutate(augmented = future_map(model, augment)) %>%
  unnest(augmented)
boot_aug
```

```{r}
boot_aug %>%
  ggplot(aes(wt, mpg)) +
  geom_line(aes(y = .fitted, group = id), alpha = 0.2, color = "#34558B") +
  geom_point(size = 2, shape = 21, stroke = 1, color = "#BD3645") +
  Yuri_theme
```

```{r}
fit_spline_on_bootstrap <- function(split) {
  data <- analysis(split)
  smooth.spline(data$wt, data$mpg, df = 4)
}

boot_splines <-
  boots %>%
  sample_n(200) %>%
  mutate(
    spline = future_map(splits, fit_spline_on_bootstrap),
    aug_train = future_map(spline, augment)
  )

splines_aug <-
  boot_splines %>%
  unnest(aug_train)

ggplot(splines_aug, aes(x, y)) +
  geom_line(aes(y = .fitted, group = id), alpha = 0.2, color = "#34558B") +
  geom_point(size = 2, shape = 21, stroke = 1, color = "#BD3645") +
  Yuri_theme
```

## 使用重采样和整洁数据的假设检验
使用灵活函数执行用于统计推断的常见假设检验。
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/gss.xlsx") -> data
```

指定变量
```{r}
data %>%
  specify(response = age)
```

```{r}
data %>%
  specify(age ~ partyid)
```

```{r}
data %>%
  specify(response = college, success = "degree")
```

## 执行统计分析-应急表的统计分析
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/ad_data.xlsx") -> ad_data
ad_data %>%
  select_mix("Genotype", "Class") %>%
  as_tibble() -> ad_data
ad_data
```

```{r}
ad_data %>%
  summarise_dt(n = .N, by = c("Genotype", "Class")) %>%
  ggplot(aes(x = Genotype, y = n, color = Class, fill = Class)) +
  geom_col(color = NA, position = position_fill()) +
  # scale_y_continuous(labels = scales::percent) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  coord_flip() +
  Yuri_theme
```

```{r}
ad_data %>%
  specify(Genotype ~ Class) %>% # 指定
  calculate(stat = "Chisq") -> observed_indep_statistic # 计算
observed_indep_statistic
```

使用随机化零分布
```{r}
ad_data %>%
  specify(Genotype ~ Class) %>% # 指定
  hypothesize(null = "independence") %>% # 假设
  generate(reps = 5000, type = "permute") %>% # 产生
  calculate(stat = "Chisq") -> null_distribution_simulated # 计算
null_distribution_simulated
```

基于理论的方法
```{r}
ad_data %>%
  specify(Genotype ~ Class) %>% # 指定
  hypothesize(null = "independence") %>% # 假设
  # 无生成步骤
  calculate(stat = "Chisq") -> null_distribution_theoretical # 计算
null_distribution_theoretical
```

观察分布
```{r}
null_distribution_simulated %>%
  visualize() +
  shade_p_value(observed_indep_statistic, direction = "greater")
```

```{r}
ad_data %>%
  specify(Genotype ~ Class) %>%
  hypothesize(null = "independence") %>%
  visualize(method = "theoretical") +
  shade_p_value(observed_indep_statistic, direction = "greater")
```

```{r}
null_distribution_simulated %>%
  visualize(method = "both") +
  shade_p_value(observed_indep_statistic,
    direction = "greater"
  )
```

计算p值
```{r}
p_value_independence <- null_distribution_simulated %>%
  get_p_value(
    obs_stat = observed_indep_statistic,
    direction = "greater"
  )
p_value_independence
```

```{r}
meta_rates <- c(
  "E2E2" = 0.71, "E2E3" = 11.4, "E2E4" = 2.32,
  "E3E3" = 61.0, "E3E4" = 22.6, "E4E4" = 2.22
)
meta_rates <- meta_rates / sum(meta_rates) # these add up to slightly > 100%

obs_rates <- table(ad_data$Genotype) / nrow(ad_data)
round(cbind(obs_rates, meta_rates) * 100, 2)
```

```{r}
ad_data %>%
  specify(response = Genotype) %>%
  hypothesize(null = "point", p = meta_rates) %>%
  calculate(stat = "Chisq") -> observed_gof_statistic
```

```{r}
ad_data %>%
  specify(response = Genotype) %>%
  hypothesize(null = "point", p = meta_rates) %>%
  generate(reps = 5000, type = "simulate") %>%
  calculate(stat = "Chisq") -> null_distribution_gof
```

```{r}
null_distribution_gof %>%
  visualize() +
  shade_p_value(observed_gof_statistic,
    direction = "greater"
  )
```

```{r}
p_value_gof <- null_distribution_gof %>%
  get_p_value(observed_gof_statistic,
    direction = "greater"
  )
```

```{r}
chisq_test(ad_data, response = Genotype, p = meta_rates)
```




## 回归模型有两种方式
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/ames.xlsx") -> data
```

```{r}
set.seed(2022)
data_split <- initial_split(data, strata = "Sale_Price", prop = 0.75)
ames_train <- training(data_split)
ames_test <- testing(data_split)
```

```{r}
rf_defaults <- rand_forest(mode = "regression")
rf_defaults
```

```{r}
preds <- c("Longitude", "Latitude", "Lot_Area", "Neighborhood", "Year_Sold")

rf_xy_fit <-
  rf_defaults %>%
  set_engine("ranger") %>%
  fit_xy(
    x = ames_train[, preds],
    y = log10(ames_train$Sale_Price)
  )

rf_xy_fit
```





















## LASSO回归
(glmnet)只接受矩阵形式的数据  
变量过多&观测数较少  
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/Breast cancer survival agec.xlsx") %>%
  na.omit() -> data
dim(data)
```

```{r}
y <- as.matrix(data[, 8]) # 结果
x <- as.matrix(data[, c(2:7, 9:11)]) # 变量
```

```{r}
f <- glmnet(x, y, family = "binomial", nlambda = 100, alpha = 1)
# alpha=1是LASSO回归,alpha=0是岭回归
```
- `family="gaussian"` 适用于一维连续因变量(univariate)
- `family="mgaussian"` 适用于多维连续因变量(multivariate)
- `family="poisson"` 适用于非负次数因变量(count)
- `family="binomial"` 适用于二元离散因变量(binary)
- `family="multinomial"` 适用于多元离散因变量(category)

```{r}
f %>%
  tidy()
```

```{r}
plot(f, xvar = "lambda", label = T)
```
横坐标为随着lambdas的对数,纵坐标为变量系数,可以看到随着lambdas增加变量系数不断减少,部分变量系数变为0(等于没有这个变量)

```{r}
predict(f, newx = x[2:5, ], type = "response") # 交叉验证
cvfit <- cv.glmnet(x, y)
plot(cvfit)
```
两条虚线,一个是均方误差最小时的λ值,一个是距离均方误差最小时一个标准误的λ值

### 提取
```{r}
tidy_df <- tidy(f)
tidy_cvdf <- tidy(cvfit)
```

```{r}
ggplot(tidy_df, aes(step, estimate, group = term, color = term)) +
  geom_line(size = 0.5) +
  geom_hline(yintercept = 0) +
  labs(y = "Coefficients") +
  scale_color_manual(values = cols) + # viridis::viridis(10)
  scale_fill_manual(values = fills) + # viridis::viridis(10)
  Yuri_theme
```

```{r}
ggplot(tidy_df, aes(lambda, estimate, group = term, color = term)) +
  geom_line(size = 0.5) +
  geom_hline(yintercept = 0) +
  scale_x_log10() +
  labs(x = "Log Lambda", y = "Coefficients") +
  scale_color_manual(values = cols) + # viridis::viridis(10)
  scale_fill_manual(values = fills) + # viridis::viridis(10)
  Yuri_theme
```

```{r}
ggplot() +
  geom_point(data = tidy_cvdf, aes(lambda, estimate)) +
  geom_errorbar(data = tidy_cvdf, aes(x = lambda, ymin = conf.low, ymax = conf.high)) +
  scale_x_log10() +
  labs(x = "Log Lambda", y = "Coefficients") +
  Yuri_theme
```

```{r}
tidy(cvfit)
cvfit$lambda.min %>% # 求出最小值
  data.table()
cvfit$lambda.1se %>% # 求出最小值一个标准误的λ值
  data.table()
```

代入测试
```{r}
l.coef2 <- coef(cvfit$glmnet.fit, s = 0.00551827, exact = F)
l.coef1 <- coef(cvfit$glmnet.fit, s = 0.04272596, exact = F)
l.coef1
l.coef2
```
第一个模型变量都没有了,第二个模型还有5个变量  

广义线性方程  
```{r}
glm(status ~ age + pathsize + lnpos + pr, family = "binomial", data = data) -> mod
summary(mod)
exp(confint(mod))
```
求OR&95%CI  

## lasso回归中分类变量的处理
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/Breast cancer survival agec.xlsx") %>%
  na.omit() -> data
dim(data)
```
age表示年龄,
lnpos表示腋窝淋巴结阳性,
status结局事件是否死亡,
pathscat表示病理肿瘤大小类别(分组变量),
time是生存时间  

把分类变量转成因子  
```{r}
data$er <- as.factor(data$er) # 雌激素受体状态
data$pr <- as.factor(data$pr) # 孕激素受体状态
data$ln_yesno <- as.factor(data$ln_yesno) # 是否有淋巴结肿大
data$histgrad <- as.factor(data$histgrad) # 病理组织学等级
data$pathscat <- as.factor(data$pathscat) # 病理肿瘤大小(厘米)
```

lasso的cox模型  
提取结局和时间  
```{r}
y <- data$status
time <- data$time
```

```{r}
data %>%
  select_mix(-c("id", "status", "time", "agec")) -> data1
data1
```

分类变量变成哑变量矩阵形式  
```{r}
model_mat <- model.matrix(~ +er + pr + ln_yesno + histgrad + pathscat - 1, data1)
```

重新组成数据x
```{r}
x <- as.matrix(data.frame(
  age = data1$age,
  pathsize = data1$pathsize,
  lnpos = data1$lnpos,
  model_mat
))
```

交叉验证
```{r}
set.seed(2022)
cv.fit <- cv.glmnet(x, Surv(time, y), family = "cox", maxit = 1000)
# cv.fit <- cv.glmnet(x, Surv(time, y), family = "cox", alpha = 1, nfolds = 10)
plot(cv.fit)
```
maxit=1000是让它迭代1000次的意思,如果迭代没到1000次,可能会出现一次报错  

```{r}
cv.fit$lambda.min
```

```{r}
fit <- glmnet(x, Surv(time, y), family = "cox", maxit = 1000)
plot(fit)
```

查看和提取系数
```{r}
Coefficients <- coef(fit, s = cv.fit$lambda.min)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
Active.Index
Active.Coefficients
```

标出了最后还剩下的变量的位置和变量的系数,对照x,没有预测功能,不能进行预测。


# 鸢尾花测试
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/iris.xlsx") -> data
data %>%
  mutate_vars(1:4, scale) -> data1
data1[, N := 1:150]
data1
data1 %>%
  select_mix(N) %>%
  as.matrix() -> m1
data1 %>%
  select_mix(-c("N", "Species")) %>%
  as.matrix() -> m2
rownames(m2) <- m1
Heatmap(m2,
  col = rev(viridis::viridis(5000)),
  column_dend_reorder = T,
  row_dend_reorder = T,
  show_column_names = T,
  show_row_names = F,
)
```

# mlr3
## k近邻算法
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/diabetes.xlsx") %>%
  data.table() -> data
data
data %>%
  ggplot(aes(x = glucose, y = insulin, color = class)) +
  geom_point(size = 2, stroke = 1, fill = NA, shape = 21) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
data %>%
  ggplot(aes(x = sspg, y = insulin, color = class)) +
  geom_point(size = 2, stroke = 1, fill = NA, shape = 21) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
data %>%
  ggplot(aes(x = sspg, y = glucose, color = class)) +
  geom_point(size = 2, stroke = 1, fill = NA, shape = 21) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
```

# 机器学习
## 监督学习
- 线性回归
- 逻辑回归
- 线型判别分析
- 决策树
- 朴素贝叶斯
- k邻近
- 学习向量量化
- 支持向量机
- 随机森林
- AdaBoost

## 非监督学习
- 高斯混合模型
- 限制波尔兹曼机
- K-means聚类
- 最大期望算法

# 简单线性回归
怀孕情况;血糖;血压;皮肤厚度;胰岛素水平;体重指数;糖尿病谱系功能;年龄;糖尿病诊断结果
```{r}
data <- import("D:/desktop/book/tidybook/data/diabetes.xlsx")
```
研究问题:根据血糖情况来预测胰岛素水平

```{r}
data %>%
  ggplot(aes(x = Glucose, y = Insulin)) +
  geom_point() +
  stat_smooth() +
  Yuri_theme
```

```{r}
data %>%
  slice_dt(Glucose > 0 & Insulin > 0) %>%
  ggplot(aes(x = Glucose, y = Insulin)) +
  geom_point() +
  stat_smooth() +
  Yuri_theme
```

```{r}
model <- lm(Insulin ~ Glucose, data = data)
model
```

回归线
```{r}
data %>%
  slice_dt(Glucose > 0 & Insulin > 0) %>%
  ggplot(aes(x = Glucose, y = Insulin)) +
  geom_point() +
  stat_smooth(method = lm) +
  Yuri_theme
```

# Logistic模型
- 寻找危险因素,找到某些影响因变量的"坏因素",一般可以通过优势比发现危险因素
- 用于预测,可以预测某种情况发生的概率或可能性大小
- 用于判别,判断某个新样本所属的类别
- 因变量为二分类变量
- 因变量和自变量之间不存在线性关系
- 没有关于自变量分布的假设条件,可以是连续变量,离散变量和虚拟变量

创建回归模型的函数:`glm(变量之间的关系的符号,数据集,指定模型的细节)`

|类型|方法|
|-:|:-|
|普通二分类logistic回归|`glm`|
|因变量多分类logistic回归|---|
|有序分类因变量|`MASS::polrb`|
|无序分类因变量|`nnet::multinom`|
|条件logistic回归|`survival::clogit`|

```{r}
x <- seq(from = -10, to = 10, by = 0.01)
y <- exp(x) / (1 + exp(x))
ggplot(data = NULL, mapping = aes(x = x, y = y)) +
  geom_line(color = "blue", size = 1)
```

# 案例1
```{r}
input <- mtcars[, c("am", "cyl", "hp", "wt")]
head(input)
m.data <- glm(formula = am ~ cyl + hp + wt, data = input, family = binomial)
print(summary(m.data))
```
在总结中,对于变量"cyl"和"hp",最后一列中的p值大于0.05,我们认为它们对变量"am"的值有贡献是无关紧要的,只有重量(wt)影响该回归模型中的"am"值

# 案例2
```{r}
data(Affairs, package = "AER")
df <- Affairs
df$ynaffairs <- ifelse(df$affairs > 0, 1, 0)
table(df$ynaffairs)
df$ynaffairs <- factor(df$ynaffairs,
  levels = c(0, 1),
  labels = c("No", "Yes")
)
table(df$ynaffairs)
fit.full <- glm(ynaffairs ~ gender + age + yearsmarried + children + religiousness + education + occupation + rating, data = df, family = binomial())
summary(fit.full)
```

根据回归系数的P值可以看到性别,是否有孩子,学历,职业对方程的贡献都不显著.去除这些变量重新拟合模型

```{r}
fit.reduced <- glm(ynaffairs ~ age + yearsmarried + religiousness + rating, data = df, family = binomial())
anova(fit.full, fit.reduced, test = "Chisq")
```

可以看到结果中p值等于0.2108大于0.05,表明四个变量和9个变量的模型拟合程度没有差别

# 接下来是评价变量对结果概率的影响
## 构造一个测试集
```{r}
testdata <- data.frame(
  rating = c(1, 2, 3, 4, 5),
  age = mean(df$age),
  yearsmarried = mean(df$yearsmarried),
  religiousness = mean(df$religiousness)
)
testdata$prob <- predict(fit.reduced,
  newdata = testdata,
  type = "response"
)
ggplot(testdata, aes(x = rating, y = prob)) +
  geom_col(aes(fill = factor(rating)), show.legend = F) +
  geom_label(aes(label = round(prob, 2))) +
  Yuri_theme
```

# 随机森林
(randomForest)
``{r}
set.seed(2022)
data("iris") -> data
trainlist <- createDataPartition(iris$Species, p = 0.8, list = F)
trainset <- iris[trainlist, ]
testset <- iris[-trainlist, ]
```

## 建模
``{r}
set.seed(2022)
rf.train <- randomForest(as.factor(Species) ~ data = trainset,
  importance = T,
  na.action = na.pass, # 略过缺失值
  ntree = 500
)

plot(rf.train, main = "randomforest origin")
rf.train
```

## 预测
``{r}
set.seed(2022)
rf.test <- predict(rf.train, newdata = testset, type = "class")
rf.cf <- caret::confusionMatrix(as.factor(rf.test), as.factor(testset$Species), )
rf.test
rf.cf
```

## ROC
``{r}
rf.test2 <- predict(rf.train, newdata = testset, type = "prob")
head(rf.test2)
roc.rf <- multiclass.roc(testset$Species, rf.test2)
roc.rf
```

randomForest()函数中的两个重要参数为ntree和mtry,其中ntree为包含的基分类器个数,默认为500;mtry为每个决策树包含的变量个数,默认为logN,数据量不大时可以循环选择最优参数值,eval = F

---
title: "tidymodels"
author: "Linze Yu"
date: "`r Sys.Date()`"
output:
  rmdformats::downcute:
    self_contained: TRUE
    default_style: "light"
    downcute_theme: "default"
    fig_width: 7
    fig_height: 5
    use_bookdown: TRUE
    gallery: TRUE
    lightbox: TRUE
    highlight: tango
editor_options: 
  markdown: 
    wrap: 72
---

```{r, 代码全局设置, echo = F}
knitr::opts_chunk$set(warning = F, error = F, message = F, prompt = F, comment = "", echo = T, dpi = 200, collapse = F, fig.align = "left", dev = "png")
# , eval = F代码显示不运行
# , include = F代码运行, 不显示代码和结果
# , echo = T 显示代码块
# , prompt = T使用>开始代码
# , comment = ""结果不使用##
# , collapse = T#代码合并结果
# , out.width/out.height = 0.8缩放
# , fig.align = "left","center","right"对齐方式
# , dev = "pdf","png","svg","jpeg"记录设备
# , cache = T
```

## 加载包
```{r 加载包}
library("YuriR") #
# library("mlr3verse") #

library("tidymodels") #
library("broom.mixed") # 用于将贝叶斯模型转换为整洁的tibbles
library("dotwhisker") # 用于可视化回归结果

# remotes::install_github("fsolt/dotwhisker")
library("rstanarm")
library("MASS") #
library("nnet") #
# library("survival")#
library("AER") #
# library("glmnet") #
library("randomForest") # 随机森林
library("caret")
library("pROC")
```

# learn
## 具有整洁数据原则的相关性和回归基础知识
(tidymodels)

## 具有整洁数据原理的 K 均值聚类(一种无监督学习)
(tidymodels)  
鸢尾花  
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/iris.xlsx") %>%
  select_mix(Sepal.Length, Sepal.Width, Species) %>%
  data.frame() -> data
```

```{r}   
data %>%
  ggplot(., aes(Sepal.Length, Sepal.Width, color = Species)) +
  geom_point(alpha = 1, size = 3, shape = 21, stroke = 1) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
```

```{r}
data %>%
  select_mix(-c("Species")) -> points
kmeans(points, centers = 3) -> kclust # 在一个矩阵上执行K均值聚类
kclust
summary(kclust) # 查看信息
kclust$cluster # 查看聚类的结果,也就是每个点聚到了那个类
kclust$centers # 查看中心点(聚类点)
augment(kclust, points) # 将点分类添加到原始数据集
tidy(kclust) %>% # 每个集群级别进行汇总
  data.table()
glance(kclust) # 提取单行摘要
```

### 探索性聚类
探索从1到24的不同选择对此聚类的影响
(furrr)
```{r}
tibble(k = 1:24) %>%
  mutate(
    kclust = future_map(k, ~ kmeans(points, .x)),
    tidied = future_map(kclust, tidy),
    glanced = future_map(kclust, glance),
    augmented = future_map(kclust, augment, points)
  ) -> kclusts
kclusts
```

```{r}
kclusts %>%
  unnest(cols = c(tidied)) -> clusters
clusters
kclusts %>%
  unnest(cols = c(augmented)) -> assignments
assignments
kclusts %>%
  unnest(cols = c(glanced)) -> clusterings
clusterings
```

```{r}
assignments %>%
  ggplot(., aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point(aes(color = .cluster), shape = 21, fill = NA, stroke = 0.5) +
  geom_point(data = clusters, size = 10, shape = "*", color = 1) +
  facet_wrap(~k, ncol = 4) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
```

```{r}
clusterings %>%
  ggplot(., aes(k, tot.withinss)) +
  geom_line(size = 2, linetype = 3, color = "#FC766A") +
  geom_point(size = 5, shape = 21, fill = NA, stroke = 2, color = "#34558B") +
  scale_y_continuous(breaks = seq(0, 200, by = 25), minor_breaks = seq(0, 200, by = 5)) +
  scale_x_continuous(breaks = seq(0, 13, by = 1), minor_breaks = seq(0, 0, by = 0)) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
```

## LASSO回归
(glmnet)只接受矩阵形式的数据  
变量过多&观测数较少  
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/Breast cancer survival agec.xlsx") %>%
  na.omit() -> data
dim(data)
```

```{r}
y <- as.matrix(data[, 8]) # 结果
x <- as.matrix(data[, c(2:7, 9:11)]) # 变量
```

```{r}
f <- glmnet(x, y, family = "binomial", nlambda = 100, alpha = 1)
# alpha=1是LASSO回归,alpha=0是岭回归
```
- `family="gaussian"` 适用于一维连续因变量(univariate)
- `family="mgaussian"` 适用于多维连续因变量(multivariate)
- `family="poisson"` 适用于非负次数因变量(count)
- `family="binomial"` 适用于二元离散因变量(binary)
- `family="multinomial"` 适用于多元离散因变量(category)

```{r}
f %>%
  tidy()
```

```{r}
plot(f, xvar = "lambda", label = T)
```
横坐标为随着lambdas的对数,纵坐标为变量系数,可以看到随着lambdas增加变量系数不断减少,部分变量系数变为0(等于没有这个变量)

```{r}
predict(f, newx = x[2:5, ], type = "response") # 交叉验证
cvfit <- cv.glmnet(x, y)
plot(cvfit)
```

两条虚线,一个是均方误差最小时的λ值,一个是距离均方误差最小时一个标准误的λ值
```{r}
tidy(cvfit)
cvfit$lambda.min %>% # 求出最小值
  data.table()
cvfit$lambda.1se %>% # 求出最小值一个标准误的λ值
  data.table()
```

代入测试
```{r}
l.coef2 <- coef(cvfit$glmnet.fit, s = 0.00551827, exact = F)
l.coef1 <- coef(cvfit$glmnet.fit, s = 0.04272596, exact = F)
l.coef1
l.coef2
```
第一个模型变量都没有了,第二个模型还有5个变量  

广义线性方程  
```{r}
glm(status ~ age + pathsize + lnpos + pr, family = "binomial", data = data) -> mod
summary(mod)
exp(confint(mod))
```
求OR&95%CI  

## lasso回归中分类变量的处理
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/Breast cancer survival agec.xlsx") %>%
  na.omit() -> data
dim(data)
```
age表示年龄,
lnpos表示腋窝淋巴结阳性,
status结局事件是否死亡,
pathscat表示病理肿瘤大小类别(分组变量),
time是生存时间  

把分类变量转成因子  
```{r}
data$er <- as.factor(data$er) # 雌激素受体状态
data$pr <- as.factor(data$pr) # 孕激素受体状态
data$ln_yesno <- as.factor(data$ln_yesno) # 是否有淋巴结肿大
data$histgrad <- as.factor(data$histgrad) # 病理组织学等级
data$pathscat <- as.factor(data$pathscat) # 病理肿瘤大小(厘米)
```

lasso的cox模型  
提取结局和时间  
```{r}
y <- data$status
time <- data$time
```

```{r}
data %>%
  select_mix(-c("id", "status", "time", "agec")) -> data1
data1
```

分类变量变成哑变量矩阵形式  
```{r}
model_mat <- model.matrix(~ +er + pr + ln_yesno + histgrad + pathscat - 1, data1)
```

重新组成数据x
```{r}
x <- as.matrix(data.frame(
  age = data1$age,
  pathsize = data1$pathsize,
  lnpos = data1$lnpos,
  model_mat
))
```

交叉验证
```{r}
set.seed(2022)
cv.fit <- cv.glmnet(x, Surv(time, y), family = "cox", maxit = 1000)
# cv.fit <- cv.glmnet(x, Surv(time, y), family = "cox", alpha = 1, nfolds = 10)
plot(cv.fit)
```
maxit=1000是让它迭代1000次的意思,如果迭代没到1000次,可能会出现一次报错  

```{r}
cv.fit$lambda.min
```

```{r}
fit <- glmnet(x, Surv(time, y), family = "cox", maxit = 1000)
plot(fit)
```

查看和提取系数
```{r}
Coefficients <- coef(fit, s = cv.fit$lambda.min)
Active.Index <- which(Coefficients != 0)
Active.Coefficients <- Coefficients[Active.Index]
Active.Index
Active.Coefficients
```

标出了最后还剩下的变量的位置和变量的系数,对照x,没有预测功能,不能进行预测。
















# 海胆
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/urchins.xlsx") %>%
  setNames(c("food_regime", "initial_volume", "width")) %>%
  mutate(food_regime = factor(food_regime, levels = c("Initial", "Low", "High"))) -> urchins
```

```{r}
urchins %>%
  ggplot(., aes(x = initial_volume, y = width, col = food_regime, fill = food_regime)) +
  geom_point(shape = 21, size = 2, stroke = 1) +
  geom_smooth(method = lm, se = F) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme +
  theme(legend.position = (c(0.2, 0.95)))
```

(parsnip)
```{r}
linear_reg()
```

```{r}
linear_reg() %>%
  set_engine("keras") # 模型引擎
```

```{r}
lm_mod <- linear_reg()
lm_fit <- lm_mod %>%
  fit(width ~ initial_volume * food_regime, data = urchins)
lm_fit
```

```{r}
tidy(lm_fit)
```

(dotwhisker)
```{r}
tidy(lm_fit) %>%
  dwplot(
    dot_args = list(size = 2, color = "#FC766A", shape = 21, stroke = 1),
    whisker_args = list(size = 1, color = "#696AAD"),
    vline = geom_vline(size = 1, xintercept = 0, colour = "#96999C", linetype = 2)
  ) +
  Yuri_theme
```

```{r}
new_points <- expand.grid(
  initial_volume = 20,
  food_regime = c("Initial", "Low", "High")
)
new_points
```

```{r}
mean_pred <- predict(lm_fit, new_data = new_points)
mean_pred
```

```{r}
conf_int_pred <- predict(lm_fit,
  new_data = new_points,
  type = "conf_int"
)
conf_int_pred
```

```{r}
plot_data <-
  new_points %>%
  bind_cols(mean_pred) %>%
  bind_cols(conf_int_pred)
```

```{r}
ggplot(plot_data, aes(x = food_regime)) +
  geom_point(aes(y = .pred)) +
  geom_errorbar(aes(
    ymin = .pred_lower,
    ymax = .pred_upper
  ),
  width = .2
  ) +
  labs(y = "urchin size")
```

```{r}
prior_dist <- rstanarm::student_t(df = 1)
set.seed(2022)
bayes_mod <- linear_reg() %>%
  set_engine("stan",
    prior_intercept = prior_dist,
    prior = prior_dist
  )
```

```{r}
bayes_fit <-
  bayes_mod %>%
  fit(width ~ initial_volume * food_regime, data = urchins)

print(bayes_fit, digits = 5)
```

```{r}
tidy(bayes_fit, conf.int = TRUE)
```

```{r}
bayes_plot_data <-
  new_points %>%
  bind_cols(predict(bayes_fit, new_data = new_points)) %>%
  bind_cols(predict(bayes_fit, new_data = new_points, type = "conf_int"))

ggplot(bayes_plot_data, aes(x = food_regime)) +
  geom_point(aes(y = .pred)) +
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) +
  labs(y = "urchin size") +
  ggtitle("Bayesian model with t(1) prior distribution")
```

```{r}
urchins %>%
  group_by(food_regime) %>%
  summarize(med_vol = median(initial_volume))
```

```{r}
bayes_mod %>%
  fit(width ~ initial_volume * food_regime, data = urchins)
```

```{r}
ggplot(
  urchins,
  aes(initial_volume, width)
) + # returns a ggplot object
  geom_jitter() + # same
  geom_smooth(method = lm, se = FALSE) + # same
  labs(x = "Volume", y = "Width")
```






























# mlr3
## k近邻算法
```{r}
import("D:/desktop/book/tidybook/data/tidymodels/diabetes.xlsx") %>%
  data.table() -> data
data
data %>%
  ggplot(., aes(x = glucose, y = insulin, color = class)) +
  geom_point(size = 2, stroke = 1, fill = NA, shape = 21) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
data %>%
  ggplot(., aes(x = sspg, y = insulin, color = class)) +
  geom_point(size = 2, stroke = 1, fill = NA, shape = 21) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
data %>%
  ggplot(., aes(x = sspg, y = glucose, color = class)) +
  geom_point(size = 2, stroke = 1, fill = NA, shape = 21) +
  scale_color_manual(values = cols) +
  scale_fill_manual(values = fills) +
  Yuri_theme
```

# 机器学习
## 监督学习
- 线性回归
- 逻辑回归
- 线型判别分析
- 决策树
- 朴素贝叶斯
- k邻近
- 学习向量量化
- 支持向量机
- 随机森林
- AdaBoost

## 非监督学习
- 高斯混合模型
- 限制波尔兹曼机
- K-means聚类
- 最大期望算法

# 简单线性回归
怀孕情况;血糖;血压;皮肤厚度;胰岛素水平;体重指数;糖尿病谱系功能;年龄;糖尿病诊断结果
```{r}
data <- import("D:/desktop/book/tidybook/data/diabetes.xlsx")
```
研究问题:根据血糖情况来预测胰岛素水平

```{r}
data %>%
  ggplot(., aes(x = Glucose, y = Insulin)) +
  geom_point() +
  stat_smooth() +
  Yuri_theme
```

```{r}
data %>%
  slice_dt(Glucose > 0 & Insulin > 0) %>%
  ggplot(., aes(x = Glucose, y = Insulin)) +
  geom_point() +
  stat_smooth() +
  Yuri_theme
```

```{r}
model <- lm(Insulin ~ Glucose, data = data)
model
```

回归线
```{r}
data %>%
  slice_dt(Glucose > 0 & Insulin > 0) %>%
  ggplot(., aes(x = Glucose, y = Insulin)) +
  geom_point() +
  stat_smooth(method = lm) +
  Yuri_theme
```

# Logistic模型
- 寻找危险因素,找到某些影响因变量的"坏因素",一般可以通过优势比发现危险因素
- 用于预测,可以预测某种情况发生的概率或可能性大小
- 用于判别,判断某个新样本所属的类别
- 因变量为二分类变量
- 因变量和自变量之间不存在线性关系
- 没有关于自变量分布的假设条件,可以是连续变量,离散变量和虚拟变量

创建回归模型的函数:`glm(变量之间的关系的符号,数据集,指定模型的细节)`

|类型|方法|
|-:|:-|
|普通二分类logistic回归|`glm`|
|因变量多分类logistic回归|---|
|有序分类因变量|`MASS::polrb`|
|无序分类因变量|`nnet::multinom`|
|条件logistic回归|`survival::clogit`|

```{r}
x <- seq(from = -10, to = 10, by = 0.01)
y <- exp(x) / (1 + exp(x))
ggplot(data = NULL, mapping = aes(x = x, y = y)) +
  geom_line(color = "blue", size = 1)
```

# 案例1
```{r}
input <- mtcars[, c("am", "cyl", "hp", "wt")]
head(input)
m.data <- glm(formula = am ~ cyl + hp + wt, data = input, family = binomial)
print(summary(m.data))
```
在总结中,对于变量"cyl"和"hp",最后一列中的p值大于0.05,我们认为它们对变量"am"的值有贡献是无关紧要的,只有重量(wt)影响该回归模型中的"am"值

# 案例2
```{r}
data(Affairs, package = "AER")
df <- Affairs
df$ynaffairs <- ifelse(df$affairs > 0, 1, 0)
table(df$ynaffairs)
df$ynaffairs <- factor(df$ynaffairs,
  levels = c(0, 1),
  labels = c("No", "Yes")
)
table(df$ynaffairs)
fit.full <- glm(ynaffairs ~ gender + age + yearsmarried + children + religiousness + education + occupation + rating, data = df, family = binomial())
summary(fit.full)
```

根据回归系数的P值可以看到性别,是否有孩子,学历,职业对方程的贡献都不显著.去除这些变量重新拟合模型

```{r}
fit.reduced <- glm(ynaffairs ~ age + yearsmarried + religiousness + rating, data = df, family = binomial())
anova(fit.full, fit.reduced, test = "Chisq")
```

可以看到结果中p值等于0.2108大于0.05,表明四个变量和9个变量的模型拟合程度没有差别

# 接下来是评价变量对结果概率的影响
## 构造一个测试集
```{r}
testdata <- data.frame(
  rating = c(1, 2, 3, 4, 5),
  age = mean(df$age),
  yearsmarried = mean(df$yearsmarried),
  religiousness = mean(df$religiousness)
)
testdata$prob <- predict(fit.reduced,
  newdata = testdata,
  type = "response"
)
ggplot(testdata, aes(x = rating, y = prob)) +
  geom_col(aes(fill = factor(rating)), show.legend = F) +
  geom_label(aes(label = round(prob, 2))) +
  Yuri_theme
```

# 随机森林
(randomForest)
```{r}
set.seed(2022)
data("iris") -> data
trainlist <- createDataPartition(iris$Species, p = 0.8, list = F)
trainset <- iris[trainlist, ]
testset <- iris[-trainlist, ]
```

## 建模
```{r}
set.seed(2022)
rf.train <- randomForest(as.factor(Species) ~ .,
  data = trainset,
  importance = T,
  na.action = na.pass, # 略过缺失值
  ntree = 500
)

plot(rf.train, main = "randomforest origin")
rf.train
```

## 预测
```{r}
set.seed(2022)
rf.test <- predict(rf.train, newdata = testset, type = "class")
rf.cf <- caret::confusionMatrix(as.factor(rf.test), as.factor(testset$Species), )
rf.test
rf.cf
```

## ROC
```{r}
rf.test2 <- predict(rf.train, newdata = testset, type = "prob")
head(rf.test2)
roc.rf <- multiclass.roc(testset$Species, rf.test2)
roc.rf
```

randomForest()函数中的两个重要参数为ntree和mtry,其中ntree为包含的基分类器个数,默认为500;mtry为每个决策树包含的变量个数,默认为logN,数据量不大时可以循环选择最优参数值,eval = F
